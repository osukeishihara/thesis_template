%!TEX root = ../thesis.tex
\chapter*{概要}
\thispagestyle{empty}
%
\begin{center}
  % \scalebox{1.5}{タイトル}\\
  \scalebox{1.0}{視覚と行動のend-to-end学習により}\\
    \vspace{-1.0zh}
    \scalebox{1.0}{経路追従行動をオンラインで模倣する手法の提案}\\
    \scalebox{1.0}{-目的地を示してトボロジカルマップからシナリオを生成-}\\
\end{center}
% \vspace{-1.0zh}

%自分の概要
多くの自律移動ロボットは，占有格子地図などのメトリックマップを基にしたナビゲーションを利用している．
本研究室の岡田らは,オドメトリや LiDAR のデータを用いたメトリックマップに基づいたナビゲーションを行い,その出力となる
目標角速度とカメラ画像を模倣学習することで，学習後はカメラ画像のみで自律移動できる手法を提案した.
また，実験により，ロボットが視覚に基づいて一定の経路を周回できることが確認されている．
春山らは，岡田らの従来手法に，環境中の分岐路を検出する機能，分岐路での適切な進行方向を提示する機能，動的に経路を選択して移動する機能を追加する
ことで，任意の目的地に向けて移動する手法を提案した．しかし，この手法では，任意の目的地に向けて移動する際に使用される，目的地までの経路を示す
指示文（以後，シナリオと呼ぶ）は，人間が作成したものが使われていた．新たな経路をロボット自身がシナリオを作成する機能が求められる．
本論文では，この機能を春山らの手法に追加し，ロボット自身にシナリオ作成をおこなわせ，任意の目的地に向けて自立移動できるかの検証をした．
はじめに，ロボット自身が作成したシナリオで任意の目的地に自立移動できるのかを，シミュレータを用いて検証した．
次に，実ロボットを用いた検証を行った．




\vspace{1zh}
キーワード: end-to-end学習 自律移動ロボット ナビゲーション
\newpage
%%
\chapter*{abstract}
\thispagestyle{empty}
\vspace{-2.0zh}
\begin{center}
  \scalebox{1.0}{A proposal for an online imitation method of}\\
  \vspace{-1zh}
  \scalebox{1.0}{path-tracking behavior by end-to-end learning of vision and action}
  \scalebox{1.0}{-Generating scenarios from a topological map}
  \scalebox{1.0}{by endicating the destination-}
\end{center}
% \vspace{-0.5zh}
Many autonomous mobile robots utilize navigation based on metric maps, 
such as occupancy grid maps. In our laboratory, we have proposed a method to 
acquire vision-based navigation by imitating actions of navigation based on 
these metric maps using vision as input. 
Furthermore, experiments have confirmed that robots can navigate a set 
course using vision as input. This method aims to acquire actions for 
following a route, where the routes for navigation were restricted to a 
fixed set. For moving towards any arbitrary destination, it is considered necessary 
to have the functionality to detect branching roads in the environment, present the 
appropriate direction at these branching points, and dynamically select and move along a route.
In this paper, we attempt to expand the route for navigation from a 
fixed course to a route towards any arbitrary destination by 
adding these functions to the conventional methods of Okada et al. 
Initially, the function to dynamically select and move along a route was added. 
For verification, experiments using a simulator were conducted, and it was confirmed that the 
system could dynamically select and navigate different routes at the same branching point in 
vision-based navigation. Subsequently, by adding the function to detect branching 
roads from vision and present the direction towards the destination, we 
constructed a system capable of autonomous movement to the destination based on vision. 
Furthermore, experiments using real robots confirmed that the constructed system could follow a 
route based on vision and reach the destination.\\
keywords: end-to-end learning, autonomous mobile robot, navigation